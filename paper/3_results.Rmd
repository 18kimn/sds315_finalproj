
# Results


### Summary statistics of response variables

Some initial summary statistics in Table 2s show a far bigger "weakness" of our study: engagement with the email to fill out the form was far too low to be able to discern a statistically significant test of whether animated graphics assist with event registration. We present three metrics here of engagement: whether students returned to the email a second time, whether students clicked on the form, whether they actually filled out the form. Only the metric of whether the students actually filled out the form would be relevant for a club aiming to boost event registration. 

As we see from these statistics, although a number of people returned to either email a second time, far fewer people actually clicked on the form and an even smaller number of people filled out the form. Trends from the three metrics also do not match each other, given the animated bar and static line treatment grop opened the follow-up email a second time at a noticeably lower rate than other groups but had the highest percentage of form clicks and form fills. 

These statistics also suggest that our study is not only hampered by an overall lack of engagement, but if anything a . Looking at the proportion of respondents who opened either the initial or the follow-up email a second time, we see that even though these groups do not suffer a problem of insufficient data, there still is not a large difference across all four groups. A difference from the control group to the both-animated group also appears, if at all, in the unexpected direction,


```{r summary_table, results = "asis"}

second_opens <- reports %>% 
  group_by( grp) %>% 
  summarize(open_freq = mean(opens_count > 1, na.rm=T))


fills <- c(4,1,2,2,2,1,3,1,1,2)
rates <-  table(fills)/table(reports$merge_fields$MMERGE7 )
rates <- tibble(set = "Follow-up", grp = unique(second_opens$grp), `Form fill rate` = as.numeric(rates)) 

clicks <- reports %>% drop_na(time_diff,clicks) %>% 
  group_by(grp) %>% 
  top_n(1, wt = time_diff) %>% 
  select(clicks) %>% 
  full_join(rates, by = "grp")

full_join(second_opens, clicks, by = "grp") %>% 
  filter(set == "Follow-up") %>% 
  select(-set) %>% 
  mutate(grp = as.character(grp), 
         across(where(is.numeric), ~ifelse(is.na(.), NA_character_, paste0(round(.*100,2), "%")))) %>% 
  rename(`Treatment Group` = grp,
         `Opened either email a second time` = open_freq,
         `Form click rate` = clicks) %>% 
  stargazer(summary = F, header = F, 
            
            title = "Summary of Response Statistics Across Treatment Groups")

```




```{r summary_chart, fig.cap = "Bar plot of response variables."}
summary_chart <- full_join(second_opens, clicks, by = "grp") %>% 
  rename(`Treatment Group` = grp,
         `Opened email a second time` = open_freq,
         `Form click rate` = clicks) %>% 
  pivot_longer(cols = c(`Opened email a second time`, `Form click rate`, `Form fill rate`)) %>% 
  ggplot(aes(fill = `Treatment Group`, x = name, y = value)) + 
  geom_bar(stat = "identity", position = "dodge") + 
  labs(fill = NULL, y = "Rate", x = NULL,
       title = "Summary statistics of response variables")
ggsave(here("figures/summary_chart.png"), summary_chart, width  =8, height = 5) 

```

### Pairwise comparisons and linear regression

Going further with a set of pairwise t-tests with Bonferroni corrections (Table 3), we confirm our suspicions that there is no significant difference in form response rates across all four treatment groups. A linear regression (Table 4) using just the control group that received only static images and the both-animated group, which performs a similar test without the Bonferroni correction, returns a similar result that animations did not have a significant impact on event registration, be it positive or negative, compared to emails without graphics. 

```{r t_test, results = "asis"}

#i can't figure out how to do this the "right" way but this hacks it well enough for our presentation
rates <- rates %>% 
  mutate(n = table(reports$merge_fields$MMERGE7)) 

dta <- tibble(id = unlist(map2(unique(rates$grp), rates$n, function(x,y) rep(x,y)))) %>% 
  group_by(id) %>% 
  mutate(counter = 1:n()) %>% 
  group_split() %>% 
  map2_dfr(c(4,4,1,1), function(x,y) mutate(x, filled = ifelse(counter <=y, 1, 0)))

# pairwise.t.test(dta$filled, dta$id, p.adjust.method = "bonf")$p.value %>% 
#   as_tibble(rownames = "Comparison Group") %>% 
#   mutate(across(where(is.numeric), ~round(., 4))) %>% 
#   rename_all(~str_wrap2(.,15))  %>% 
#   stargazer(header = F, summary = F,
#             title = "P-values from pairwise comparisons using t-tests, pooled SD, and Bonferroni corrections")


```

\begin{table}[!htbp] \centering 
  \caption{P-values from pairwise comparisons using t-tests, pooled SD, and Bonferroni corrections} 
  \label{} 
\begin{tabular}{@{\extracolsep{5pt}} ccccc} 
\\[-1.8ex]\hline 
\hline \\[-1.8ex] 
 & \shortstack{Comparison\\ Group} & \shortstack{Control (No\\ animation)} & \shortstack{Animated Bar,\\ Static Line} & \shortstack{Static Bar,\\ Animated Line} \\ 
\hline \\[-1.8ex] 
1 & Animated Bar, Static Line & 1 & NA & NA \\ 
2 & Static Bar, Animated Line & 1 & 0.8995 & NA \\ 
3 & Both Animated & 1 & 0.892 & 1 \\ 
\hline \\[-1.8ex] 
\end{tabular} 
\end{table} 

```{r lm, results = "asis"}
dta2 <- dta %>% filter(id %in% c("Control (No animation)", "Both Animated")) 
dta3 <- dta %>% filter(id %in% c("Static Bar, Animated Line", "Animated Bar, Static Line")) 


mod1 <- lm(filled ~ id, data=dta2 ) 
#mod2 <- lm(filled ~ id, data= dta3)
stargazer(mod1,  header = F,
          covariate.labels = "Treatment Group = Both Animated",
            dep.var.caption = "", 
            dep.var.labels = "Fill rate", type = "latex", 
            title = "Model-based comparison among just animated and static (control) treatment groups.") 
  


```

### Power analysis

Evidently, our research design did not find a significant difference between the animation group and the control group. But instead of being a problem of how our treatment was implemented, it could have been simply an issue of not having enough participants. We purposefully cast quite a large net (the entire first year class, a total of 1,265 students) to avoid this issue, but maybe a future experiment could go even larger and thus find significance. We ran a power analysis to find how increasing sample size could help our search for power, specifically testing fifteen sample sizes from 2,000 to 20,000 participants and a hypothesized treatment effect of .006, with 200 simulated experiments each. 


```{r, fig.width = 7, fig.height =4, fig.cap = "Power analysis."}

#Commented out because the simulations take a while to run -- instead I just ran it once, then saved it, so that
#when we knit the Rmd later the simulations don't have to be run again and we can just use the results from that run. 

# possible.ns <- seq(from=2000, to=20000, length.out = 15) # The sample sizes we'll be considering
# powers <- rep(NA, length(possible.ns))           # Empty object to collect simulation estimates
# alpha <- 0.05                                    # Standard significance level
# sims <- 200                                      # Number of simulations to conduct for each N
# tau <- .006                                      # Hypothesize CACE treatment effect
# baseline.vote.rate <- 0.008                        # The expected voter turnout rate in placebo/control
# 
# #### Outer loop to vary the number of subjects ####
# for (j in 1:length(possible.ns)){
#   N <- possible.ns[j]                              # Pick the jth value for N
#   
#   significant.experiments <- rep(NA, sims)         # Empty object to count significant experiments
#   message(N)
#   #### Inner loop to conduct experiments "sims" times over for each N ####
#   for (i in 1:sims){
#     Y0 <-  rbinom(N, 1, baseline.vote.rate)        # control potential outcome
#     Y1 <- rbinom(N, 1, baseline.vote.rate + tau)   # treatment potential outcome
#     Z.sim <- rbinom(n=N, size=1, prob=.5)          # Do a random assignment
#     Y.sim <- Y1*Z.sim + Y0*(1-Z.sim)               # Reveal outcomes according to assignment
#     fit.sim <- lm(Y.sim ~ Z.sim)                   # Do analysis (Simple regression)
#     p.value <- summary(fit.sim)$coefficients[2,4]  # Extract p-values
#     significant.experiments[i] <- (p.value <= alpha) # Determine significance according to p <= 0.05
#   }
#   
#   powers[j] <- mean(significant.experiments)       # store average success rate (power) for each N
# }
# tibble(possible.ns, powers) %>% 
#   saveRDS(here("data/powers.RDS"))
powers <- readRDS(here("data/powers.RDS"))

powers %>% 
  ggplot(aes(x = possible.ns, y = powers)) + 
  geom_point(color = "steelblue4") + 
  geom_hline(yintercept = .8, color = "red") + 
  labs(x = "Sample size tested",y = "Expected power", 
       title=  "Would increasing sample size help our experiment?",
       subtitle = "Sort of, but there are bigger problems at hand.",
       caption = "200 simulations per sample size were run, with a hypothetical CACE treatment effect of .006") +
  theme_lato() + 
  theme(panel.grid.minor = element_blank(), 
        axis.title.y = element_text(hjust=  0.5, family = "Lato"), 
        panel.border =  element_rect(size = 1, fill = NA)) + 
  ggsave(here("figures/power.png"), width = 8, height = 5)



```


As we see in Figure 4, although brute-forcing the experiment and increasing sample size would eventually lead us to a power above .8, such an experiment would likely require around 10,000 participants. In addition to being practically difficult, it also likely would still be theoretically strange (as the current observed effect favors static images over animated images in terms of student engagement and event registration) and the small magnitude in this experiment would mean this effect is probably inconsequential even if statistically significant. Instead of increasing the sample size, future experiments on this question should change research design as noted below. 


